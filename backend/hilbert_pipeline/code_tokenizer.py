# =============================================================================
# code_tokenizer.py â€” Code-aware tokenizer for Hilbert LSA pipeline
# =============================================================================
#
# Designed as a plug-in replacement for raw token extraction inside
# hilbert_pipeline.lsa_layer. Handles:
#   - Python syntax noise suppression (keywords, builtins, operators)
#   - Docstring/comment prioritisation
#   - Identifier splitting (snake_case, camelCase, path fragments)
#   - Normalised domain tokens (hilbert_x, lsa_field, molecule_df, etc.)
#
# Public API:
#     tokens = tokenize_code_string(source_text)
#
# =============================================================================

from __future__ import annotations
import re
import keyword
import builtins
from typing import List


# -------------------------------------------------------------------------
# Noise dictionaries
# -------------------------------------------------------------------------

PY_KEYWORDS = set(keyword.kwlist)
PY_BUILTINS = set(dir(builtins))

# Common operators and punctuation seen in code
OP_TOKENS = {
    "=", "+", "-", "*", "/", "%", "==", "!=", "<", ">", "<=", ">=",
    "(", ")", "{", "}", "[", "]", ".", ",", ":", ";", "->", "=>"
}

# Pure noise identifiers often generated by tooling
NOISE_IDENTIFIERS = {
    "self", "cls", "args", "kwargs",
    "tmp", "temp", "var", "val",
    "__init__", "__main__", "__name__", "__file__"
}

# Domain-preserving whitelisted short tokens
SHORT_WHITELIST = {
    "lsa", "pdf", "ocr", "hilbert", "api", "df", "nlp"
}


# -------------------------------------------------------------------------
# Regex patterns
# -------------------------------------------------------------------------

# Comments and docstrings
RE_COMMENT = re.compile(r"#.*$")
RE_DOCSTRING = re.compile(r'(""".*?"""|\'\'\'.*?\'\'\')', re.DOTALL)

# Split snake_case, camelCase, and dotted identifiers
RE_SPLIT = re.compile(
    r"[A-Za-z][a-z]+|[A-Z][a-z]+|[A-Z]+(?=[A-Z][a-z]|$)|\d+"
)

# Paths (Windows + POSIX)
RE_PATH = re.compile(r"([A-Za-z]:)?[\\/][A-Za-z0-9_./\\-]+")


# -------------------------------------------------------------------------
# Utility filters
# -------------------------------------------------------------------------

def _clean_fragment(tok: str) -> str:
    """General normalisation of identifier fragments."""
    tok = tok.strip().lower()

    if not tok:
        return ""

    # strip file extensions
    for ext in (".py", ".txt", ".csv", ".json"):
        if tok.endswith(ext):
            tok = tok[: -len(ext)]

    # remove leftover path symbols
    tok = tok.replace("\\", " ").replace("/", " ")

    return tok


def _should_keep(tok: str) -> bool:
    """Decide whether a token is meaningful enough to keep."""
    if not tok:
        return False

    low = tok.lower()

    # Drop python keywords/builtins
    if low in PY_KEYWORDS or low in PY_BUILTINS:
        return False

    if low in OP_TOKENS:
        return False

    if low in NOISE_IDENTIFIERS:
        return False

    # Drop very short tokens unless whitelisted
    if len(low) < 3 and low not in SHORT_WHITELIST:
        return False

    # Drop pure numeric and hex-like
    if low.isdigit():
        return False

    return True


# -------------------------------------------------------------------------
# Main tokenizer
# -------------------------------------------------------------------------

def tokenize_code_string(text: str) -> List[str]:
    """
    Extract meaningful domain tokens from Python-like source code.

    Steps:
      1. Extract docstrings + comments first with priority.
      2. Remove comments/docstrings from main body.
      3. Split identifiers into semantic fragments.
      4. Flatten and filter using meaningfulness rules.
    """
    if not text:
        return []

    # ---- 1. Extract docstrings + comments ----
    docstrings = RE_DOCSTRING.findall(text)
    comments = RE_COMMENT.findall(text)

    priority_text = "\n".join(docstrings + comments)

    # ---- 2. Remove comments/docstrings from main body ----
    cleaned = RE_DOCSTRING.sub(" ", text)
    cleaned = RE_COMMENT.sub(" ", cleaned)

    # ---- 3. Remove paths (they introduce tons of entropy) ----
    cleaned = RE_PATH.sub(" ", cleaned)

    # ---- 4. Tokenize priority text (heavier weight in downstream LSA) ----
    priority_tokens = _split_and_normalise(priority_text)

    # ---- 5. Tokenize cleaned code ----
    code_tokens = _split_and_normalise(cleaned)

    return priority_tokens + code_tokens


def _split_and_normalise(text: str) -> List[str]:
    """Split identifiers into semantic components and normalise."""
    toks: List[str] = []

    for raw in re.split(r"[^A-Za-z0-9_]+", text):
        if not raw:
            continue

        # camelCase / snake_case splitting
        parts = RE_SPLIT.findall(raw)
        if not parts:
            parts = [raw]

        for p in parts:
            f = _clean_fragment(p)
            if _should_keep(f):
                toks.append(f)

    return toks
