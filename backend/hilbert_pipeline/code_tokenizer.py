# =============================================================================
# hilbert_pipeline/code_tokenizer.py â€” Code-aware tokenizer for Hilbert LSA
# =============================================================================
#
# Designed as a plug-in replacement for raw token extraction inside
# hilbert_pipeline.lsa_layer. Handles:
#   - Python syntax noise suppression (keywords, builtins, operators)
#   - Docstring/comment prioritisation
#   - Identifier splitting (snake_case, camelCase, path fragments)
#   - Normalised domain tokens (hilbert_x, lsa_field, molecule_df, etc.)
#
# Public API (backwards compatible):
#     tokens = tokenize_code_string(source_text)
#     tokens = tokenize_code_string(source_text, language="python")
#
# The `language` parameter is optional and currently advisory:
#   - "python" (default) uses full docstring + "#" comment extraction.
#   - other languages fall back to generic identifier tokenisation but still
#     benefit from splitting and noise filters.
# =============================================================================

from __future__ import annotations

import re
import keyword
import builtins
from typing import List, Optional


# -------------------------------------------------------------------------
# Noise dictionaries
# -------------------------------------------------------------------------

PY_KEYWORDS = set(keyword.kwlist)
PY_BUILTINS = set(dir(builtins))

# Common operators and punctuation seen in code
OP_TOKENS = {
    "=", "+", "-", "*", "/", "%", "==", "!=", "<", ">", "<=", ">=",
    "(", ")", "{", "}", "[", "]", ".", ",", ":", ";", "->", "=>",
}

# Pure noise identifiers often generated by tooling
NOISE_IDENTIFIERS = {
    "self", "cls", "args", "kwargs",
    "tmp", "temp", "var", "val",
    "__init__", "__main__", "__name__", "__file__",
}

# Domain-preserving whitelisted short tokens
SHORT_WHITELIST = {
    "lsa", "pdf", "ocr", "hilbert", "api", "df", "nlp",
}


# -------------------------------------------------------------------------
# Regex patterns
# -------------------------------------------------------------------------

# Python comments and docstrings
RE_PY_COMMENT = re.compile(r"#.*$", re.MULTILINE)
RE_PY_DOCSTRING = re.compile(r'(""".*?"""|\'\'\'.*?\'\'\')', re.DOTALL)

# Split snake_case, camelCase, and mixed identifiers into fragments
RE_SPLIT = re.compile(
    r"[A-Za-z][a-z]+|[A-Z][a-z]+|[A-Z]+(?=[A-Z][a-z]|$)|\d+"
)

# Paths (Windows + POSIX)
RE_PATH = re.compile(r"([A-Za-z]:)?[\\/][A-Za-z0-9_./\\-]+")


# -------------------------------------------------------------------------
# Utility filters
# -------------------------------------------------------------------------

def _clean_fragment(tok: str) -> str:
    """General normalisation of identifier fragments."""
    tok = tok.strip().lower()
    if not tok:
        return ""

    # strip common file extensions
    for ext in (".py", ".txt", ".csv", ".json"):
        if tok.endswith(ext):
            tok = tok[: -len(ext)]

    # remove leftover path separators
    tok = tok.replace("\\", " ").replace("/", " ")

    return tok.strip()


def _should_keep(tok: str) -> bool:
    """Decide whether a token is meaningful enough to keep."""
    if not tok:
        return False

    low = tok.lower()

    # Drop Python keywords and builtins
    if low in PY_KEYWORDS or low in PY_BUILTINS:
        return False

    # Drop operator-like tokens
    if low in OP_TOKENS:
        return False

    # Drop common noise identifiers
    if low in NOISE_IDENTIFIERS:
        return False

    # Drop very short tokens unless whitelisted
    if len(low) < 3 and low not in SHORT_WHITELIST:
        return False

    # Drop pure numeric tokens
    if low.isdigit():
        return False

    return True


def _split_and_normalise(text: str) -> List[str]:
    """Split identifiers into semantic components and normalise."""
    toks: List[str] = []

    # Rough token candidates separated by non-alnum/underscore
    for raw in re.split(r"[^A-Za-z0-9_]+", text):
        if not raw:
            continue

        # camelCase / snake_case splitting
        parts = RE_SPLIT.findall(raw)
        if not parts:
            parts = [raw]

        for p in parts:
            f = _clean_fragment(p)
            if _should_keep(f):
                toks.append(f)

    return toks


# -------------------------------------------------------------------------
# Main tokenizer
# -------------------------------------------------------------------------

def tokenize_code_string(text: str, language: Optional[str] = None) -> List[str]:
    """
    Extract meaningful domain tokens from source code.

    Parameters
    ----------
    text : str
        Raw source code string.
    language : str or None
        Advisory language hint, e.g. "python", "c", "java".
        Currently used to decide how aggressively to look for docstrings
        and comments. Non-python languages fall back to generic tokenisation.

    Returns
    -------
    tokens : list of str
        Normalised, domain-bearing code tokens suitable for LSA.
    """
    if not text:
        return []

    lang = (language or "python").lower()

    # ------------------------------------------------------------------
    # 1. Extract docstrings and comments with priority (Python only)
    # ------------------------------------------------------------------
    docstrings: List[str] = []
    comments: List[str] = []

    if lang in ("python", "py"):
        docstrings = RE_PY_DOCSTRING.findall(text)
        comments = RE_PY_COMMENT.findall(text)
    else:
        # For other languages, the LSA layer already extracts comments
        # via separate text pipelines. Here we treat the whole source
        # uniformly and do not try to parse comment syntax.
        docstrings = []
        comments = []

    priority_text = "\n".join(docstrings + comments).strip()

    # ------------------------------------------------------------------
    # 2. Remove docstrings/comments from main body (Python) and paths
    # ------------------------------------------------------------------
    if lang in ("python", "py"):
        cleaned = RE_PY_DOCSTRING.sub(" ", text)
        cleaned = RE_PY_COMMENT.sub(" ", cleaned)
    else:
        cleaned = text

    # Remove paths that introduce a lot of entropy
    cleaned = RE_PATH.sub(" ", cleaned)

    # ------------------------------------------------------------------
    # 3. Tokenise priority text (comments + docstrings)
    # ------------------------------------------------------------------
    priority_tokens: List[str] = []
    if priority_text:
        priority_tokens = _split_and_normalise(priority_text)

    # ------------------------------------------------------------------
    # 4. Tokenise cleaned code body
    # ------------------------------------------------------------------
    code_tokens = _split_and_normalise(cleaned)

    # Order: priority tokens first, then body tokens
    return priority_tokens + code_tokens


__all__ = ["tokenize_code_string"]
